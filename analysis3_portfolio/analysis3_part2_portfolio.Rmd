---
title: "analysis3_part2"
author: "C. Coleman"
date: "10/19/2021"
output: html_document
---

# Fit model analysis!

_In this section, we will be using the clean_df data to split, train, and assess the fit model of flu-like symptom data completed in the last module._

_As such, we will be using the Tidymodels package to continue practicing data analysis._

</br>

_As always, I like to start by loading needed packages. Our core packages for this specific exercise will be here(), dpylr(), and tidymodels(). Additionally, I like to load tidyverse() just because I use it fairly often._
```{r}
library(tidyverse) #Working with multiple Tidy packages
library(tidymodels) #Building models
library(dplyr) #data manipulation
library(here) #setting pathways for saving files
library(rpart) #Model fitting
library(ranger) #Model fitting
library(glmnet) #Model fitting
```


## Data Splitting and Training

</br>

_Next, we need to load in out data. We are going to pull from the processed data folder using the here() function. Note that while we are reading in the clean_df data frame, we will be calling it "df" in this exercise._
```{r}
#Next two lines load data frame for the linear and logistic regression analysis for the analysis3 portion of this MADA exercise.
clean_df_location <- here::here("Files", "processeddata.RDS")

clean_data2 <- readRDS(clean_df_location)

#These two lines are for loading an additional cleaned data frame used for the machine learning portion of this MADA analysis.
clean_df_location3 <- here::here("Files", "processeddata3.RDS")

clean_data3 <- readRDS(clean_df_location3)
```

</br>

 _Now we are set to start our data splitting/training._
 
 _The first step is splitting our data into a training data set and a test data set. The training set will contain most of the original data (~75%), while the test data set will have a smaller portion (~25%). The training data is used to fit a model and the test data is to assess how good of a fit the data is._
```{r}
# Split 3/4 of the data into training data 
data_split <- initial_split(clean_data2, prop = 3/4)

# Make new data frames for training and test data
train_data <- training(data_split)
test_data  <- testing(data_split)
```
 
 </br>
 
## Assessing fit model for Nausea and all other outcomes.
 
 </br>
 
#### Creating a Recipe and Workflow

</br>

_At this stage, our data is split into training and test data sets. Now, we need to create a recipe and workflow to help process the train data for model building. The output is a function that will run the entire logistic regression model for any data set. Therefore, we can run the same model for the train and test data frames to create the exact same analysis workflow to ensure comparison_
```{r}
#Creates recipe
Recipe_Nausea <- recipe(Nausea ~ ., data = train_data)

#Define logistical regression model pipe
log_mod <- logistic_reg() %>% 
  set_engine("glm")

#Create workflow that adds our recipe and model
Nausea_glm_wflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(Recipe_Nausea)
```

</br>

#### Modeling Using the Workflow

</br>

_Using the workflow above, lets not fit the model to the train data set (Recipe_Nausea)._
```{r}
#Defining a command that runs model fitting to Recipe_Nausea
Nausea_fit <- 
  Nausea_glm_wflow %>% 
  fit(data = train_data)

#Pull log reg fit model using parsnip()
Nausea_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

</br>

### Using Workflows to Make Predictions

</br>

_Now that we have our modeling workflow, we are going to use the fitted model to predict values in the test data set._
```{r}
#predict values in test data
predict(Nausea_fit, test_data)
```

</br>

_Additionally, we can use the augment() function to predict outcomes in the test data. But unlike the predict() function, augment() indludes prediction residuals._
```{r}
#Predict outcomes using augment in test data
Nausea_aug_test <- 
  augment(Nausea_fit, test_data)
```

</br>

### Using ROC and ROC_AUC to Assess Model Fit

</br>

_At this point, we have a fit model, a workflow for that model, and have some predictions made from a test subset of our data. Now, we need to used to test predictions to assess if the model can predict values that reflect our actual data. In particular, we will be using the the *ROC* curve as our metric with *ROC_AUC* to measure area under the ROC curve that our model intersects._

#### Assessing fit for test data predictions

</br>
```{r}
#Make ROC curve for test data predictions and calculate area under the curve
Nausea_aug_test %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_aug_test %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```
</br>

```{r}
#Predict outcomes using augment in training data
Nausea_aug_train <- 
  augment(Nausea_fit, train_data)

#Make ROC curve for training data and calculate area under the curve
Nausea_aug_train %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_aug_train %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```

</br>

_The resulting ROC_AUC values were 0.68 and 0.79 for the test data model and the training data model, respectively. This suggests that the models are good fits to our data and can predict outcomes._

</br>

### Assessing fit model for Nausea and Runny Nose.

 </br>
 
#### Creating a Recipe and Workflow

</br>

_Create a recipe and workflow to help process the train data for model building._
```{r}
#Creates recipe
Nausea_RN_Recipe <- recipe(Nausea ~ RunnyNose, data = train_data)

#Define logistical regression model pipe
log_mod <- logistic_reg() %>% 
  set_engine("glm")

#Create workflow that adds our recipe and model
Naus_RN_glm_wflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(Nausea_RN_Recipe)
```

</br>

#### Modeling Using the Workflow

</br>

_Use workflow to fit the model to the train data set (Nausea_RN_Recipe)._
```{r}
#Defining a command that runs model fitting to Nausea_RN_Recipe
Nausea_RN_fit <- 
  Naus_RN_glm_wflow %>% 
  fit(data = train_data)

#Pull log reg fit model using parsnip()
Nausea_RN_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

</br>

### Using Workflows to Make Predictions

</br>

_Use the fitted model to predict values in the test data set._
```{r}
#predict values in test data
predict(Nausea_RN_fit, test_data)
```

</br>

_Use the augment() function to predict outcomes in the test data._
```{r}
#Predict outcomes using augment in test data
Nausea_RN_aug_test <- 
  augment(Nausea_RN_fit, test_data)
```

</br>

### Using ROC and ROC_AUC to Assess Model Fit

</br>

#### Assessing fit for test data predictions

</br>
```{r}
#Make ROC curve for test data predictions and calculate area under the curve
Nausea_RN_aug_test %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_RN_aug_test %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```
</br>

```{r}
#Predict outcomes using augment in training data
Nausea_RN_aug_train <- 
  augment(Nausea_RN_fit, train_data)

#Make ROC curve for training data and calculate area under the curve
Nausea_RN_aug_train %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_RN_aug_train %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```

</br>

_The resulting ROC_AUC values were 0.48 and 0.52. for the test data model and the training data model, respectively. This suggests that the test data was not fit well by the model, but the model had a decent fit to train data, but not a great fit. Therefore, since both ROC_AUC values are ~0.5, it is okay to make predictions using the model, but use caution._





## Linear Modeling with Body Temperature as the Outcome. Contributor: Priyanka


### Model Continous Outcomes


_Creating the recipe for BodyTemp vs all predictors_

```{r}
BodyTemp.recipe <- recipe(BodyTemp ~ ., data = train_data)
BodyTemp.recipe
```


_Fitting a linear model_


_Setting up the linear model_

```{r}
lr.mod <-  linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")
```

_Create workflow that adds our recipe and model_

```{r}
BodyTemp.wflow <- 
  workflow() %>% 
  add_model(lr.mod) %>% 
  add_recipe(BodyTemp.recipe)
```


```{r}
BodyTemp.wflow
```

_Use workflow to fit the model to the train data set_

```{r}
BodyTemp_fit <- 
  BodyTemp.wflow %>% 
  fit(data = train_data)
```  
  
  
_To view a tibble_

```{r}
BodyTemp_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```  
  
#### Using Workflows to Make Predictions_

_predict values in test data_

```{r}
predict(BodyTemp_fit, test_data)
``` 

_Using augment() function to predict outcomes in the test data._

```{r}
BodyTemp.aug <- 
  augment(BodyTemp_fit, test_data)
  
# The data look like:

BodyTemp.aug %>%
  select(BodyTemp, .pred)

```

_Model Evaluation With RMSE_

```{r}
BodyTemp.aug %>% 
  rmse(truth = BodyTemp, .pred)
  
#rmse = 1.12 

```

### Model with just the main predictor: Runnynose_

```{r}
BT_RN_recipe <- recipe(BodyTemp ~ RunnyNose, data = train_data)

BT_RN_recipe

```

### Fitting a linear model


_Setting up the linear model_

```{r}
lr.mod1 <-  linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")
```

_Create workflow that adds our recipe and model_

```{r}
BT_RN_wflow <- 
  workflow() %>% 
  add_model(lr.mod1) %>% 
  add_recipe(BT_RN_recipe)
```

_Use workflow to fit the model to the train data set_

```{r}
BT_RN_fit <- 
  BT_RN_wflow %>% 
  fit(data = train_data)
``` 

_To view a tibble_

```{r}
BT_RN_fit %>%
  extract_fit_parsnip() %>%
  tidy()

BT_RN_fit

```  

### Using Workflows to Make Predictions

_predict values in test data_

```{r}
predict(BT_RN_fit, test_data)
``` 

_Using augment() function to predict outcomes in the test data._

```{r}
BT_RN_aug <- 
  augment(BT_RN_fit, test_data)
  
# The data look like:

BT_RN_aug %>%
  select(BodyTemp, .pred)

```

### Model Evaluation With RMSE

```{r}
BT_RN_aug %>% 
  rmse(truth = BodyTemp, .pred)
  
#rmse = 1.10

```

_Looking at the output, the model with all the predictor is better than model with just runny nose as a predictor._

# Machine Learning Modeling: 11/1/2021; Carter Coleman

</br>

## Data Set Up:

</br>

_Set seed: This sets a random number generator with initial (pseudo)random values set as "123". We will need a series of random numbers created for our machine learning analysis._
```{r}
set.seed(123)
```

 _Test/Train data split: Split data into a training data set and a test data set. The training set will contain most of the original data (70%), while the test data set will have a smaller portion (30%). The training data is used to fit a model and the test data is to assess how good of a fit the data is._
```{r}
# Split 3/4 of the data into training data 
data_split2 <- initial_split(clean_data3, prop = 7/10, strata = BodyTemp)

# Make new data frames for training and test data
train_data2 <- training(data_split2)
test_data2  <- testing(data_split2)
```

</br>

## Train Data: Body Temperature Null Model

_5-fold cross validation, 5 times repeated for train data: Here we are setting a cross-validation of the machine learning models. Cross-validation is used to measure how the results of our machine learning models will generalize to an independent data set. As such, the folds created will be be 5 random sub-samples of the train data set to test the validity of our models within the train data set. The 5x5 structure is arbitrary._
```{r}
fold_BT <- vfold_cv(train_data2, v = 5, repeats = 5, strata = BodyTemp)
```

_Creating the recipe for BodyTemp vs all predictors_

```{r}
BodyTemp.recipe2 <- 
  recipe(BodyTemp ~ ., data = train_data2) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())

BodyTemp.recipe2
```
_Setting linear regression model to assess relationship between body temperature (outcome) and all other predictor variables._
```{r}
lm_mod <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")
```

_However, first we need to create our null model to test against._

</br>

### Null Model:

 </br>

_Creates null model recipe. When we call this term, it will indicate in our workflow that body temperature will be predicted by a value of 1 (NULL)._ 
```{r}
Null_recipe_lm_train <- recipe(BodyTemp ~ 1, data = train_data2)
```

_Creating the Workflow: this creates a set workflow for running a null linear regression model with body temperature as the outcome._
```{r}
null_wf <- workflow() %>% add_model(lm_mod) %>% add_recipe(Null_recipe_lm_train)
```

_Here, I am going to fit the null model created in the above workflow to the folds made from the train data set._
```{r}
null_train_lm <- fit_resamples(null_wf, resamples = fold_BT)
```
_Calculate RMSE for the train data linear model._
```{r}
Null_Train_Met <- collect_metrics(null_train_lm)

Null_Train_Met
```
_RMSE = 1.21, with a standard deviation of 0.018. This will serve as our check to test our models against latter on.


</br>

## Test Data: Body Temperature Null Model

_5-fold cross validation, 5 times repeated for test data: Here we are setting a cross-validation of the machine learning models. Cross-validation is used to measure how the results of our machine learning models will generalize to an independent data set. As such, the folds created will be be 5 random sub-samples of the test data set to test the validity of our models within the test data set. The 5x5 structure is arbitrary._
```{r}
fold_BT_test <- vfold_cv(test_data2, v = 5, repeats = 5, strata = BodyTemp)
```

_Creating the recipe for BodyTemp vs all other variables as predictors._

```{r}
BodyTemp.recipe2_test <- 
  recipe(BodyTemp ~ ., data = test_data2) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())

BodyTemp.recipe2_test
```

_Setting linear regression model to assess relationship between body temperature (outcome) and all other predictor variables. This is already set above in a universal form._
```{r}
lm_mod <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")
```

_Now, we need to create our null model to test against._

</br>

### Null Model: Test Data

 </br>

_Creates null model recipe. When we call this term, it will indicate in our workflow that body temperature will be predicted by a value of 1 (NULL)._ 
```{r}
Null_recipe_lm_test <- recipe(BodyTemp ~ 1, data = test_data2)
```

_Creating the Workflow: this creates a set workflow for running a null linear regression model with body temperature as the outcome._
```{r}
null_wf_test <- workflow() %>% add_model(lm_mod) %>% add_recipe(Null_recipe_lm_test)
```

_Here, I am going to fit the null model created in the above workflow to the folds made from the train data set._
```{r}
null_lm_test <- fit_resamples(null_wf_test, resamples = fold_BT_test)
```
_Calculate RMSE for the train data linear model._
```{r}
Null_test_Met <- collect_metrics(null_lm_test)
```

</br>

# Model Tuning and Fitting

### The following section sevrves three functions:
1) Fit a Tree Model to our data using Body Temperature as the outcome of interest.
2) Fit a LASSO Model to our data using Body Temperature as the outcome of interest.
3) Fit a Tree Model to our data using Body Temperature as the outcome of interest.

</br>

## Tree Model

</br>

_Specifying The Model: Decision Tree_
```{r}
#Identifying hyperparameters we want to use.
tune_spec_dtree <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

tune_spec_dtree
```
</br>

_Tune Grid Specification: Decision Tree_
```{r}
#create a regular grid of values for using convenience functions for each hyperparameter.
tree_grid_dtree <-
  dials::grid_regular(
    cost_complexity(), 
    tree_depth(), 
    levels = 5)

tree_grid_dtree
```

</br>

_Creating a Workflow: Decision Tree_
```{r}
dtree_wf <- workflow() %>%
  add_model(tune_spec_dtree) %>%
  add_recipe(BodyTemp.recipe2)
```

</br>

_Cross Validation with tunegrid(): Decision Tree_
```{r}
dtree_resample <- 
  dtree_wf %>% 
  tune_grid(
    resamples = fold_BT,
    grid = tree_grid_dtree
    )

dtree_resample

dtree_resample %>%
  collect_metrics()
```

</br>

_Plot model performance using autoplot()_
```{r}
dtree_resample %>%
  autoplot()
```
</br>

_Showing and selecting best performing Models_
```{r}
#Showing best performing tree models
dtree_resample %>%
  show_best(n=1)
  
#Selects best performing model
best_tree <- dtree_resample %>%
  select_best()
```

_This shows that one of the shortest trees (depth = 1) is the best performing models (RMSE = 1.19; STE = 0.018). However, it doesn't really perform any better than the null model, making it a bad fit to the data._

</br>

_Creating final fit based on best model permutation and plotting predicted values from that final fit model_
```{r}
dtree_final_wf <- 
  dtree_wf %>% 
  finalize_workflow(best_tree)

dtree_final_wf

#Create workflow for fitting model to train_data2 predictions
dtree_final_fit <- 
  dtree_final_wf %>%
  fit(train_data2) 
```

#### Calculating residuals and ploting Actual vs. Predicted values

</br>
_Calculating residuals: In the Tidymodels tutorial used to do this exercise, I could not get the built in functions to cooperate (looking at you autoplot()). This is because autoplot() cannot use the output of any predict functions. Therefore, below is how we do it manually._
```{r}
dtree_residuals <- dtree_final_fit %>%
  augment(train_data2) %>% #use augment() to make predictions from train data
  select(c(.pred, BodyTemp)) %>%
  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.

dtree_residuals
```
_model predictions from tuned model vs actual outcomes_
```{r}
dtree_pred_plot <- ggplot(dtree_residuals, 
                          aes(x = BodyTemp, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: Decision Tree", 
       x = "Body Temperature Outcome", 
       y = "Body Temperature Prediction")
dtree_pred_plot
```
_plot residuals vs predictions_
```{r}
dtree_residual_plot <- ggplot(dtree_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: Decision Tree", 
       x = "Body Temperature Prediction", 
       y = "Residuals")
plot(dtree_residual_plot) #view plot
```

</br>

## LASSO Model

</br>

_Specifying The Model: LASSO_
```{r}
lasso_mod <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

</br>

_Creating a Workflow: LASSO_
```{r}
lasso_wf <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(BodyTemp.recipe2)
```

</br>

_Create Tuning Grid: LASSO_
```{r}
lasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))
```

</br>

_Cross Validation with tune_grid(): LASSO_
```{r}
lasso_resample <- 
  lasso_wf %>%
  tune_grid(resamples = fold_BT,
            grid = lasso_grid,
            control = control_grid(verbose = FALSE, save_pred = TRUE),
            metrics = metric_set(rmse))

lasso_resample %>%
  collect_metrics()
```

</br>

_Plot model performance using autoplot()_
```{r}
#Plot of actual train_data2
lasso_resample %>%
  autoplot()
```

_Showing and selecting best performing Models_
```{r}
#Showing best performing tree models
lasso_resample %>%
  show_best()
  
#Selects best performing model
best_lasso <- lasso_resample %>%
  select_best()
```

_This shows that model 18 is the best performing models (RMSE = 1.16; STE = 0.017). However, it doesn't really perform any better than the null model, making it a bad fit to the data._

_Creating final fit based on best model permutation and plotting predicted values from that final fit model_
```{r}
lasso_final_wf <- 
  lasso_wf %>% 
  finalize_workflow(best_lasso)

lasso_final_wf

#Create workflow for fitting model to train_data2 predictions
lasso_final_fit <- 
  lasso_final_wf %>%
  fit(train_data2) 
```

_Calculating residuals: In the Tidymodels tutorial used to do this exercise, I could not get the built in functions to cooperate (looking at you autoplot()). This is because autoplot() cannot use the output of any predict functions. Therefore, below is how we do it manually._
```{r}
lasso_residuals <- lasso_final_fit %>%
  augment(train_data2) %>% #use augment() to make predictions from train data
  select(c(.pred, BodyTemp)) %>%
  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.

lasso_residuals
```
_model predictions from tuned model vs actual outcomes_
```{r}
lasso_pred_plot <- ggplot(lasso_residuals, 
                          aes(x = BodyTemp, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: LASSO", 
       x = "Body Temperature Outcome", 
       y = "Body Temperature Prediction")
lasso_pred_plot
```
_plot residuals vs predictions_
```{r}
lasso_residual_plot <- ggplot(lasso_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: LASSO", 
       x = "Body Temperature Prediction", 
       y = "Residuals")
plot(lasso_residual_plot) #view plot
```

</br>

## Random Forest

</br>

_Create function to detect cores for Random Forest Model computation_
```{r}
cores <- parallel::detectCores()
cores
```
_Specifying The Model: Random Forest_
```{r}
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")
```

</br>

_Creating a Workflow: Random Forest_
```{r}
rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(BodyTemp.recipe2)
```

</br>

_Create Tuning Grid: Random Forest_
```{r}
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )
```

</br>

_Cross Validation with tune_grid(): Random Forest_
```{r}
rf_resample <- 
  rf_wf %>% 
  tune_grid(fold_BT,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

rf_resample %>%
  collect_metrics()
```

</br>

_Plot model performance using autoplot()_
```{r}
#Plot of actual train_data2
rf_resample %>%
  autoplot()
```

_Showing and selecting best performing Models_
```{r}
#Showing best performing tree models
rf_resample %>%
  show_best()
  
#Selects best performing model
best_rf <- rf_resample %>%
  select_best()
```

_This shows that "mtry 4" is the best performing models (RMSE = 1.16; STE = 0.016). However, it doesn't really perform any better than the null model, making it a bad fit to the data._

_Creating final fit based on best model permutation and plotting predicted values from that final fit model_
```{r}
rf_final_wf <- 
  rf_wf %>% 
  finalize_workflow(best_rf)

rf_final_wf

#Create workflow for fitting model to train_data2 predictions
rf_final_fit <- 
  rf_final_wf %>%
  fit(train_data2) 
```

_Calculating residuals: In the Tidymodels tutorial used to do this exercise, I could not get the built in functions to cooperate (looking at you autoplot()). This is because autoplot() cannot use the output of any predict functions. Therefore, below is how we do it manually._
```{r}
rf_residuals <- rf_final_fit %>%
  augment(train_data2) %>% #use augment() to make predictions from train data
  select(c(.pred, BodyTemp)) %>%
  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.

rf_residuals
```
_model predictions from tuned model vs actual outcomes_
```{r}
rf_pred_plot <- ggplot(rf_residuals, 
                          aes(x = BodyTemp, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: Random Forest", 
       x = "Body Temperature Actual", 
       y = "Body Temperature Prediction")
rf_pred_plot
```
_plot residuals vs predictions_
```{r}
rf_residual_plot <- ggplot(rf_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: Random Forest", 
       x = "Body Temperature Prediction", 
       y = "Residuals")
plot(rf_residual_plot) #view plot
```

</br>

# Model selection: Discussion of fit
_Based on reported RMSE alone, all the models performed pretty equally compared to the null model. However, when we made predictions and calculated residuals, visually, you could actually see some sort of relationship between actual and predicated body temperature data for the LASSO and random forest models. Therefore, I would like to exclude the decision tree based on that criteria. Finally, I would like to choose LASSO, because the random forest does not keep every single model set when running multiple permutations. Rather, it takes a random subset of "trees". This adds some bias into the fit (in this case it may over-fit), but the advantage is that it is quicker to run. In this case, LASSO is "more accurate" and slower, but the time it took to run these models weren't that drastic.

</br>

# Final evaluation

_To give the LASSO a final evaluation, we will try fitting it to the test data and compare to the train data. This will allow us to see if model performance is consistent for body temperature data._
```{r}
#fit to test data
lasso_last_fit <- 
  lasso_final_wf %>% 
  last_fit(data_split2)

lasso_last_fit %>% 
  collect_metrics()
```



