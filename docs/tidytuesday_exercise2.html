<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Tidy Tuesday Exercise 2</title>

<script src="site_libs/header-attrs-2.10/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  { color: #cccccc; background-color: #303030; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ffcfaf; } /* Alert */
code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #dca3a3; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #f0dfaf; } /* ControlFlow */
code span.ch { color: #dca3a3; } /* Char */
code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code span.co { color: #7f9f7f; } /* Comment */
code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code span.do { color: #7f9f7f; } /* Documentation */
code span.dt { color: #dfdfbf; } /* DataType */
code span.dv { color: #dcdccc; } /* DecVal */
code span.er { color: #c3bf9f; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #c0bed1; } /* Float */
code span.fu { color: #efef8f; } /* Function */
code span.im { } /* Import */
code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
code span.kw { color: #f0dfaf; } /* Keyword */
code span.op { color: #f0efd0; } /* Operator */
code span.ot { color: #efef8f; } /* Other */
code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code span.sc { color: #dca3a3; } /* SpecialChar */
code span.ss { color: #cc9393; } /* SpecialString */
code span.st { color: #cc9393; } /* String */
code span.va { } /* Variable */
code span.vs { color: #cc9393; } /* VerbatimString */
code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>







<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">My Data Analysis Portfolio</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="./aboutme.html">About Me</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./rcoding.html">R Coding</a>
    </li>
    <li>
      <a href="./visualization.html">Visualization</a>
    </li>
    <li>
      <a href="./tidytuesday.html">Tidy Tuesday</a>
    </li>
    <li>
      <a href="./tidytuesday_exercise2.html">Tidy Tuesday 2 Exercise</a>
    </li>
    <li>
      <a href="./analysis3_part2_portfolio.html">MADA Analysis 3</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/CarterColeman/CARTERCOLEMAN-MADA-portfolio">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Tidy Tuesday Exercise 2</h1>

</div>


<div id="welcome-to-my-second-tidytuesday-exercise-2.-this-week-focuses-on-a-full-analysis-of-marble-racing-data." class="section level1">
<h1>Welcome to my Second TidyTuesday Exercise 2. This week focuses on a full analysis of Marble Racing data.</h1>
<p></br></p>
<p><em>Load needed packages.</em></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co">#Working with multiple Tidy packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels) <span class="co">#Building models</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr) <span class="co">#data manipulation</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(here) <span class="co">#setting pathways for saving files</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart) <span class="co">#Model fitting</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger) <span class="co">#Model fitting</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet) <span class="co">#Model fitting</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(purrr) <span class="co">#Ensamble Modeling</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stacks)</span></code></pre></div>
<p></br></p>
<p><em>Load in the data from the TidyTuesday github.</em></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>marbles <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="st">&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-02/marbles.csv&#39;</span>)</span></code></pre></div>
<pre><code>## Rows: 256 Columns: 14</code></pre>
<pre><code>## -- Column specification --------------------------------------------------------
## Delimiter: &quot;,&quot;
## chr (9): date, race, site, source, marble_name, team_name, pole, host, notes
## dbl (5): time_s, points, track_length_m, number_laps, avg_time_lap</code></pre>
<pre><code>## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<p></br></p>
<div id="initial-data-wrangling-and-exploration" class="section level2">
<h2>Initial Data Wrangling and Exploration</h2>
<p></br></p>
<p><em>Next, I want to see whats going on in the raw data frame. We will get variable classes and summary stats using the summary() function and use the glimpse() function to get a sense of numbe of rows/variables. The main goal is to see what might be useful, interesting, and studiable from thr marble racing data set.</em></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(marbles)</span></code></pre></div>
<pre><code>##      date               race               site              source         
##  Length:256         Length:256         Length:256         Length:256        
##  Class :character   Class :character   Class :character   Class :character  
##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
##                                                                             
##                                                                             
##                                                                             
##                                                                             
##  marble_name         team_name             time_s           pole          
##  Length:256         Length:256         Min.   : 17.76   Length:256        
##  Class :character   Class :character   1st Qu.: 28.40   Class :character  
##  Mode  :character   Mode  :character   Median : 36.28   Mode  :character  
##                                        Mean   :190.84                     
##                                        3rd Qu.:338.16                     
##                                        Max.   :492.01                     
##                                        NA&#39;s   :3                          
##      points       track_length_m   number_laps     avg_time_lap  
##  Min.   : 0.000   Min.   :11.90   Min.   : 1.00   Min.   :17.76  
##  1st Qu.: 0.000   1st Qu.:12.62   1st Qu.: 1.00   1st Qu.:25.94  
##  Median : 3.000   Median :13.02   Median : 5.00   Median :30.05  
##  Mean   : 6.453   Mean   :13.22   Mean   : 6.25   Mean   :29.70  
##  3rd Qu.:11.250   3rd Qu.:14.13   3rd Qu.:10.25   3rd Qu.:33.65  
##  Max.   :26.000   Max.   :14.55   Max.   :16.00   Max.   :41.62  
##  NA&#39;s   :128                                      NA&#39;s   :3      
##      host              notes          
##  Length:256         Length:256        
##  Class :character   Class :character  
##  Mode  :character   Mode  :character  
##                                       
##                                       
##                                       
## </code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(marbles)</span></code></pre></div>
<pre><code>## Rows: 256
## Columns: 14
## $ date           &lt;chr&gt; &quot;15-Feb-20&quot;, &quot;15-Feb-20&quot;, &quot;15-Feb-20&quot;, &quot;15-Feb-20&quot;, &quot;15~
## $ race           &lt;chr&gt; &quot;S1Q1&quot;, &quot;S1Q1&quot;, &quot;S1Q1&quot;, &quot;S1Q1&quot;, &quot;S1Q1&quot;, &quot;S1Q1&quot;, &quot;S1Q1&quot;,~
## $ site           &lt;chr&gt; &quot;Savage Speedway&quot;, &quot;Savage Speedway&quot;, &quot;Savage Speedway&quot;~
## $ source         &lt;chr&gt; &quot;https://youtu.be/JtsQ_UydjEI?t=356&quot;, &quot;https://youtu.be~
## $ marble_name    &lt;chr&gt; &quot;Clementin&quot;, &quot;Starry&quot;, &quot;Momo&quot;, &quot;Yellow&quot;, &quot;Snowy&quot;, &quot;Razz~
## $ team_name      &lt;chr&gt; &quot;O&#39;rangers&quot;, &quot;Team Galactic&quot;, &quot;Team Momo&quot;, &quot;Mellow Yell~
## $ time_s         &lt;dbl&gt; 28.11, 28.37, 28.40, 28.70, 28.71, 28.72, 28.96, 29.11,~
## $ pole           &lt;chr&gt; &quot;P1&quot;, &quot;P2&quot;, &quot;P3&quot;, &quot;P4&quot;, &quot;P5&quot;, &quot;P6&quot;, &quot;P7&quot;, &quot;P8&quot;, &quot;P9&quot;, &quot;~
## $ points         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
## $ track_length_m &lt;dbl&gt; 12.81, 12.81, 12.81, 12.81, 12.81, 12.81, 12.81, 12.81,~
## $ number_laps    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10,~
## $ avg_time_lap   &lt;dbl&gt; 28.11, 28.37, 28.40, 28.70, 28.71, 28.72, 28.96, 29.11,~
## $ host           &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;~
## $ notes          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~</code></pre>
<p></br></p>
<p><em>Based on the summary of glimpse of the “marbles” df, it looks like there are 256 rows (excellent, large n) and 14 variables. By viewing the df, I am also seeing that there are a number of variables we don’t need and we can get rid of immediately (“source” and “notes”). On that same thought process, I want variables that will potentially tell me something about what variables influence whether a marble wins. Therefore, I will want to use average marble lap speed as my outcome variable. I will arbitrarily select variables as predictors and start exploring relationships below.</em></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#using select() to remove variables into a clean_df</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>clean_df <span class="ot">&lt;-</span> marbles <span class="sc">%&gt;%</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="st">&quot;date&quot;</span>, <span class="sc">-</span><span class="st">&quot;race&quot;</span>, <span class="sc">-</span><span class="st">&quot;source&quot;</span>, <span class="sc">-</span><span class="st">&quot;time_s&quot;</span>, <span class="sc">-</span><span class="st">&quot;pole&quot;</span>, <span class="sc">-</span><span class="st">&quot;points&quot;</span>, <span class="sc">-</span><span class="st">&quot;notes&quot;</span>, <span class="sc">-</span><span class="st">&quot;host&quot;</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">#use glimpse() function to look out the clean_df summary meta data</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(clean_df)</span></code></pre></div>
<pre><code>## Rows: 256
## Columns: 6
## $ site           &lt;chr&gt; &quot;Savage Speedway&quot;, &quot;Savage Speedway&quot;, &quot;Savage Speedway&quot;~
## $ marble_name    &lt;chr&gt; &quot;Clementin&quot;, &quot;Starry&quot;, &quot;Momo&quot;, &quot;Yellow&quot;, &quot;Snowy&quot;, &quot;Razz~
## $ team_name      &lt;chr&gt; &quot;O&#39;rangers&quot;, &quot;Team Galactic&quot;, &quot;Team Momo&quot;, &quot;Mellow Yell~
## $ track_length_m &lt;dbl&gt; 12.81, 12.81, 12.81, 12.81, 12.81, 12.81, 12.81, 12.81,~
## $ number_laps    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10,~
## $ avg_time_lap   &lt;dbl&gt; 28.11, 28.37, 28.40, 28.70, 28.71, 28.72, 28.96, 29.11,~</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">#checking for NA variables</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">is.na.data.frame</span>(clean_df)</span></code></pre></div>
<pre><code>##         site marble_name team_name track_length_m number_laps avg_time_lap
##   [1,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [2,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [3,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [4,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [5,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [6,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [7,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [8,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [9,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [10,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [11,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [12,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [13,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [14,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [15,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [16,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [17,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [18,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [19,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [20,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [21,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [22,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [23,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [24,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [25,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [26,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [27,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [28,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [29,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [30,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [31,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [32,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [33,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [34,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [35,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [36,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [37,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [38,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [39,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [40,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [41,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [42,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [43,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [44,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [45,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [46,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [47,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [48,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [49,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [50,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [51,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [52,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [53,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [54,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [55,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [56,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [57,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [58,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [59,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [60,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [61,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [62,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [63,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [64,] FALSE       FALSE     FALSE          FALSE       FALSE         TRUE
##  [65,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [66,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [67,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [68,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [69,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [70,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [71,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [72,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [73,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [74,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [75,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [76,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [77,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [78,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [79,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [80,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [81,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [82,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [83,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [84,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [85,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [86,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [87,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [88,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [89,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [90,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [91,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [92,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [93,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [94,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [95,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [96,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [97,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [98,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [99,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [100,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [101,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [102,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [103,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [104,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [105,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [106,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [107,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [108,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [109,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [110,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [111,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [112,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [113,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [114,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [115,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [116,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [117,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [118,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [119,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [120,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [121,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [122,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [123,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [124,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [125,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [126,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [127,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [128,] FALSE       FALSE     FALSE          FALSE       FALSE         TRUE
## [129,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [130,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [131,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [132,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [133,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [134,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [135,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [136,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [137,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [138,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [139,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [140,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [141,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [142,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [143,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [144,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [145,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [146,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [147,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [148,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [149,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [150,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [151,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [152,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [153,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [154,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [155,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [156,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [157,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [158,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [159,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [160,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [161,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [162,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [163,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [164,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [165,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [166,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [167,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [168,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [169,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [170,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [171,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [172,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [173,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [174,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [175,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [176,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [177,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [178,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [179,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [180,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [181,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [182,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [183,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [184,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [185,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [186,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [187,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [188,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [189,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [190,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [191,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [192,] FALSE       FALSE     FALSE          FALSE       FALSE         TRUE
## [193,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [194,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [195,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [196,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [197,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [198,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [199,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [200,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [201,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [202,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [203,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [204,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [205,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [206,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [207,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [208,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [209,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [210,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [211,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [212,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [213,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [214,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [215,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [216,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [217,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [218,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [219,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [220,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [221,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [222,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [223,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [224,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [225,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [226,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [227,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [228,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [229,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [230,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [231,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [232,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [233,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [234,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [235,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [236,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [237,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [238,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [239,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [240,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [241,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [242,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [243,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [244,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [245,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [246,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [247,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [248,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [249,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [250,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [251,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [252,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [253,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [254,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [255,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [256,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE</code></pre>
<p></br></p>
<p><em>There are now 6 variables, with a single NA in the clean_df. For ease we are just going to remove the row with the NA. Afterwards, we can start partition the clean_df and exploring relationships.</em></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#remove row with NA</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>clean_df <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(clean_df)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">is.na.data.frame</span>(clean_df)</span></code></pre></div>
<pre><code>##         site marble_name team_name track_length_m number_laps avg_time_lap
##   [1,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [2,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [3,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [4,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [5,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [6,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [7,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [8,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##   [9,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [10,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [11,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [12,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [13,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [14,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [15,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [16,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [17,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [18,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [19,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [20,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [21,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [22,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [23,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [24,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [25,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [26,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [27,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [28,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [29,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [30,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [31,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [32,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [33,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [34,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [35,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [36,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [37,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [38,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [39,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [40,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [41,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [42,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [43,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [44,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [45,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [46,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [47,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [48,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [49,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [50,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [51,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [52,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [53,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [54,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [55,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [56,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [57,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [58,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [59,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [60,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [61,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [62,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [63,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [64,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [65,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [66,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [67,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [68,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [69,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [70,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [71,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [72,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [73,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [74,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [75,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [76,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [77,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [78,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [79,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [80,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [81,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [82,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [83,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [84,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [85,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [86,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [87,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [88,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [89,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [90,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [91,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [92,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [93,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [94,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [95,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [96,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [97,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [98,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
##  [99,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [100,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [101,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [102,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [103,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [104,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [105,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [106,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [107,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [108,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [109,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [110,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [111,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [112,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [113,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [114,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [115,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [116,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [117,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [118,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [119,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [120,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [121,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [122,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [123,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [124,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [125,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [126,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [127,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [128,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [129,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [130,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [131,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [132,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [133,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [134,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [135,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [136,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [137,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [138,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [139,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [140,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [141,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [142,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [143,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [144,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [145,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [146,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [147,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [148,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [149,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [150,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [151,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [152,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [153,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [154,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [155,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [156,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [157,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [158,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [159,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [160,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [161,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [162,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [163,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [164,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [165,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [166,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [167,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [168,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [169,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [170,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [171,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [172,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [173,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [174,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [175,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [176,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [177,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [178,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [179,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [180,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [181,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [182,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [183,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [184,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [185,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [186,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [187,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [188,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [189,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [190,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [191,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [192,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [193,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [194,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [195,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [196,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [197,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [198,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [199,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [200,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [201,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [202,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [203,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [204,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [205,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [206,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [207,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [208,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [209,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [210,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [211,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [212,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [213,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [214,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [215,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [216,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [217,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [218,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [219,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [220,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [221,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [222,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [223,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [224,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [225,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [226,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [227,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [228,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [229,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [230,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [231,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [232,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [233,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [234,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [235,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [236,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [237,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [238,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [239,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [240,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [241,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [242,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [243,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [244,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [245,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [246,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [247,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [248,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [249,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [250,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [251,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [252,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE
## [253,] FALSE       FALSE     FALSE          FALSE       FALSE        FALSE</code></pre>
<p></br></p>
<p><em>For ease, I am going to make a new df for each of the categorical variables being used. That way we can group by each variable later on to assess thinks like which team, marble, course has the fastest times.</em></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Site</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>site_df <span class="ot">&lt;-</span> clean_df <span class="sc">%&gt;%</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="st">&quot;marble_name&quot;</span>, <span class="sc">-</span><span class="st">&quot;team_name&quot;</span>, <span class="sc">-</span><span class="st">&quot;number_laps&quot;</span>, <span class="sc">-</span><span class="st">&quot;track_length_m&quot;</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Consolidate duplicates and average mean lap time per site.</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>site_df <span class="ot">&lt;-</span> site_df <span class="sc">%&gt;%</span>                                    </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(site) <span class="sc">%&gt;%</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">avg_time_lap =</span> <span class="fu">mean</span>(avg_time_lap)) <span class="sc">%&gt;%</span> </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Marble Name</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>marble_name_df <span class="ot">&lt;-</span> clean_df <span class="sc">%&gt;%</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="st">&quot;site&quot;</span>, <span class="sc">-</span><span class="st">&quot;team_name&quot;</span>, <span class="sc">-</span><span class="st">&quot;number_laps&quot;</span>, <span class="sc">-</span><span class="st">&quot;track_length_m&quot;</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">#Consolidate duplicates and average mean lap time per marble.</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>marble_name_df <span class="ot">&lt;-</span> marble_name_df <span class="sc">%&gt;%</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(marble_name) <span class="sc">%&gt;%</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">avg_time_lap =</span> <span class="fu">mean</span>(avg_time_lap)) <span class="sc">%&gt;%</span> </span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co">#Team Name</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>team_name_df <span class="ot">&lt;-</span> clean_df <span class="sc">%&gt;%</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="st">&quot;marble_name&quot;</span>, <span class="sc">-</span><span class="st">&quot;site&quot;</span>, <span class="sc">-</span><span class="st">&quot;number_laps&quot;</span>, <span class="sc">-</span><span class="st">&quot;track_length_m&quot;</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">#Consolidate duplicates and average mean lap time per team.</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>team_name_df <span class="ot">&lt;-</span> team_name_df <span class="sc">%&gt;%</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(team_name) <span class="sc">%&gt;%</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">avg_time_lap =</span> <span class="fu">mean</span>(avg_time_lap)) <span class="sc">%&gt;%</span> </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span></code></pre></div>
<p></br></p>
<p><em>Plotting avg_time_lap by Site to see if the course makes marbles particularly faster</em></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>site_plot <span class="ot">&lt;-</span> site_df <span class="sc">%&gt;%</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>site, <span class="at">y=</span>avg_time_lap, <span class="at">fill=</span>site)) <span class="sc">+</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Site&quot;</span>) <span class="sc">+</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Average Lap Time (s)&quot;</span>) <span class="sc">+</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Site vs. Average Lap Time&quot;</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(site_plot)</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-7-1.png" width="960" /> <em>Maybe Greenstone, O’raceway, Razzway, and Savage Speedway do better on average? Remember, average lap time is measure in seconds, so with the range being 20s, that is a pretty big difference.</em></p>
<p></br></p>
<p><em>Plotting avg_time_lap by marble name to see if the owner/marble itself makes a difference</em></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>marble_name_plot <span class="ot">&lt;-</span> marble_name_df <span class="sc">%&gt;%</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>marble_name, <span class="at">y=</span>avg_time_lap, <span class="at">fill=</span>marble_name)) <span class="sc">+</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Marble Name&quot;</span>) <span class="sc">+</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Average Lap Time (s)&quot;</span>) <span class="sc">+</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Marble Name vs. Average Lap Time&quot;</span>) <span class="sc">+</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(marble_name_plot)</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-8-1.png" width="768" /> <em>Also seems like some marbles tend to perform better.</em></p>
<p></br></p>
<p><em>Plotting avg_time_lap by team name to see if certain teams were just more successful than others</em></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>team_name_plot <span class="ot">&lt;-</span> team_name_df <span class="sc">%&gt;%</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>team_name, <span class="at">y=</span>avg_time_lap, <span class="at">fill=</span>team_name)) <span class="sc">+</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Team&quot;</span>) <span class="sc">+</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Average Lap Time (s)&quot;</span>) <span class="sc">+</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Team vs. Average Lap Time&quot;</span>) <span class="sc">+</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>))</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(team_name_plot)</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-9-1.png" width="960" /></p>
<p><em>Interesting! Does not seem like one team performs better than another.</em></p>
<p></br></p>
<p><em>Plotting avg_time_lap by track length and number of laps to see if the physical layout of a track enhanced marble performance.</em></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Track Length Plot</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>track_length_plot <span class="ot">&lt;-</span> clean_df <span class="sc">%&gt;%</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>track_length_m, <span class="at">y=</span>avg_time_lap)) <span class="sc">+</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se=</span><span class="cn">TRUE</span>, <span class="at">fill=</span><span class="st">&quot;orange&quot;</span>, <span class="at">color=</span><span class="st">&quot;red&quot;</span>)<span class="sc">+</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;track length&quot;</span>) <span class="sc">+</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Average Lap Time (s)&quot;</span>) <span class="sc">+</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Track length vs. Average Lap Time&quot;</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(track_length_plot)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-10-1.png" width="960" /></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Number of Laps Plot</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>laps_plot <span class="ot">&lt;-</span> clean_df <span class="sc">%&gt;%</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>number_laps, <span class="at">y=</span>avg_time_lap)) <span class="sc">+</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se=</span><span class="cn">TRUE</span>, <span class="at">fill=</span><span class="st">&quot;orange&quot;</span>, <span class="at">color=</span><span class="st">&quot;red&quot;</span>)<span class="sc">+</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Number of Laps in Race&quot;</span>) <span class="sc">+</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Average Lap Time (s)&quot;</span>) <span class="sc">+</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Number of Laps vs. Average Lap Time&quot;</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(laps_plot)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-10-2.png" width="960" /></p>
<p><em>As one would expect, the longer the track length, the larger average race time. However, number of laps does not seem to have a relationship between average lap time.</em></p>
<p></br></p>
</div>
<div id="hypothesis-and-outcome-of-interest" class="section level2">
<h2>Hypothesis and Outcome of Interest</h2>
<p><em>Based on the exploration above, I would like to use average lap time as the outcome variable in an effort to assess how different track and marble characteristics predict marble race performance. To the effect, I hypothesize that the best predictors of average marble lap time race performance will be track ID and number of laps. This is because some tracks may be graded and constructed differently, allowing for differential performance between just the tracks. Additionally, I hypothesize that because the main propelling force is gravity, and the acceleration of gravity is constant, the more laps are included in the average lap time measurement, the slower a marble’s average lap speed will be. This is because to only other forces applied to the marble other than gravity actually act to slow the marble down, which will integrate by distance traveled.</em></p>
<p></br></p>
</div>
<div id="traintest-splitting-and-additional-data-cleaning" class="section level2">
<h2>Train/Test Splitting and Additional Data Cleaning</h2>
<div id="traintest-splitting" class="section level3">
<h3>Train/Test Splitting:</h3>
<p><em>Split clean2_df into a training data set and a test data set by a proportion of 75% train to 25% test . The training data is used to fit a model and the test data is to assess how good of a fit the data is.</em></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split 3/4 of the data into training data </span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>data_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(clean_df, <span class="at">prop =</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Make new data frames for training and test data</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">training</span>(data_split)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>test_data  <span class="ot">&lt;-</span> <span class="fu">testing</span>(data_split)</span></code></pre></div>
<p></br></p>
</div>
</div>
<div id="machine-learning-models" class="section level2">
<h2>Machine Learning Models</h2>
<p><em>First, set a seed: This sets a random number generator with initial (pseudo)random values set as “123”. We will need a series of random numbers created for our machine learning analysis.</em></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span></code></pre></div>
<p></br></p>
<div id="train-data-average-lap-time-null-model" class="section level3">
<h3>Train Data: Average Lap Time Null Model</h3>
<p><em>5-fold cross validation, 5 times repeated for train data: Here we are setting a cross-validation to measure how the results of our machine learning models will generalize to an independent data set. As such, the folds created will be be 5 random sub-samples of the train data set to test the validity of our models within the train data set. The 5x5 structure is arbitrary.</em></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>fold <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(train_data, <span class="at">v =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">5</span>, <span class="at">strata =</span> avg_time_lap)</span></code></pre></div>
<p><em>Creating the recipe for Average Lap Time vs all predictors</em></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>ATL.recipe <span class="ot">&lt;-</span> </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">recipe</span>(avg_time_lap <span class="sc">~</span> ., <span class="at">data =</span> train_data) <span class="sc">%&gt;%</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_zv</span>(<span class="fu">all_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>ATL.recipe</span></code></pre></div>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          5
## 
## Operations:
## 
## Dummy variables from all_nominal(), -all_outcomes()
## Zero variance filter on all_predictors()
## Centering and scaling for all_predictors()</code></pre>
<p><em>Setting linear regression model to assess relationship between average lap time and all other predictor variables.</em></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>lm_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span> <span class="fu">set_engine</span>(<span class="st">&quot;lm&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<p><em>However, first we need to create our null model to test against.</em></p>
<p></br></p>
</div>
<div id="null-model" class="section level3">
<h3>Null Model:</h3>
<p></br></p>
<p><em>Creates null model recipe. When we call this term, it will indicate in our workflow that average lap time will be predicted by a value of 1 (NULL).</em></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>Null_recipe_lm_train <span class="ot">&lt;-</span> <span class="fu">recipe</span>(avg_time_lap <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> train_data)</span></code></pre></div>
<p><em>Creating the Workflow: this creates a set workflow for running a null linear regression model with Average Lap Time as the outcome.</em></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>null_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span> <span class="fu">add_model</span>(lm_mod) <span class="sc">%&gt;%</span> <span class="fu">add_recipe</span>(Null_recipe_lm_train)</span></code></pre></div>
<p><em>Here, I am going to fit the null model created in the above workflow to the folds made from the train data set.</em></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>null_train_lm <span class="ot">&lt;-</span> <span class="fu">fit_resamples</span>(null_wf, <span class="at">resamples =</span> fold)</span></code></pre></div>
<pre><code>## ! Fold1, Repeat1: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold2, Repeat1: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold3, Repeat1: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold4, Repeat1: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold5, Repeat1: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold1, Repeat2: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold2, Repeat2: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold3, Repeat2: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold4, Repeat2: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold5, Repeat2: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold1, Repeat3: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold2, Repeat3: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold3, Repeat3: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold4, Repeat3: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold5, Repeat3: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold1, Repeat4: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold2, Repeat4: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold3, Repeat4: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold4, Repeat4: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold5, Repeat4: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold1, Repeat5: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold2, Repeat5: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold3, Repeat5: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold4, Repeat5: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<pre><code>## ! Fold5, Repeat5: internal: A correlation computation is required, but `estimate` is const...</code></pre>
<p><em>Calculate RMSE for the train data linear model.</em></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>Null_Train_Met <span class="ot">&lt;-</span> <span class="fu">collect_metrics</span>(null_train_lm)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>Null_Train_Met</span></code></pre></div>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard     5.56    25  0.0790 Preprocessor1_Model1
## 2 rsq     standard   NaN        0 NA      Preprocessor1_Model1</code></pre>
<p>_RMSE = 5.48, with a standard deviation of 0.062. This will serve as our check to test our models against latter on.</p>
<p></br></p>
</div>
<div id="model-tuning-and-fitting" class="section level3">
<h3>Model Tuning and Fitting</h3>
<p><em>a) Fit a Tree Model</em> <em>b) Fit a LASSO Model</em> <em>c) Fit a Random Forest Model</em> <em>d) Fit a </em></p>
<p></br></p>
<div id="a-tree-model" class="section level4">
<h4>a) Tree Model</h4>
<p></br></p>
<p><em>Specifying The Model: Decision Tree</em></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Identifying hyperparameters we want to use.</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>tune_spec_dtree <span class="ot">&lt;-</span> </span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">decision_tree</span>(</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>tune_spec_dtree</span></code></pre></div>
<pre><code>## Decision Tree Model Specification (regression)
## 
## Main Arguments:
##   cost_complexity = tune()
##   tree_depth = tune()
## 
## Computational engine: rpart</code></pre>
<p></br></p>
<p><em>Tune Grid Specification: Decision Tree</em></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create a regular grid of values for using convenience functions for each hyperparameter.</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>tree_grid_dtree <span class="ot">&lt;-</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>  dials<span class="sc">::</span><span class="fu">grid_regular</span>(</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cost_complexity</span>(), </span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tree_depth</span>(), </span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>tree_grid_dtree</span></code></pre></div>
<pre><code>## # A tibble: 25 x 2
##    cost_complexity tree_depth
##              &lt;dbl&gt;      &lt;int&gt;
##  1    0.0000000001          1
##  2    0.0000000178          1
##  3    0.00000316            1
##  4    0.000562              1
##  5    0.1                   1
##  6    0.0000000001          4
##  7    0.0000000178          4
##  8    0.00000316            4
##  9    0.000562              4
## 10    0.1                   4
## # ... with 15 more rows</code></pre>
<p></br></p>
<p><em>Creating a Workflow: Decision Tree</em></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>dtree_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(tune_spec_dtree) <span class="sc">%&gt;%</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(ATL.recipe)</span></code></pre></div>
<p></br></p>
<p><em>Cross Validation with tunegrid(): Decision Tree</em></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>dtree_resample <span class="ot">&lt;-</span> </span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>  dtree_wf <span class="sc">%&gt;%</span> </span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> fold,</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> tree_grid_dtree</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>dtree_resample <span class="sc">%&gt;%</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 50 x 8
##    cost_complexity tree_depth .metric .estimator  mean     n std_err .config    
##              &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      
##  1    0.0000000001          1 rmse    standard   4.24     25  0.0601 Preprocess~
##  2    0.0000000001          1 rsq     standard   0.431    25  0.0105 Preprocess~
##  3    0.0000000178          1 rmse    standard   4.24     25  0.0601 Preprocess~
##  4    0.0000000178          1 rsq     standard   0.431    25  0.0105 Preprocess~
##  5    0.00000316            1 rmse    standard   4.24     25  0.0601 Preprocess~
##  6    0.00000316            1 rsq     standard   0.431    25  0.0105 Preprocess~
##  7    0.000562              1 rmse    standard   4.24     25  0.0601 Preprocess~
##  8    0.000562              1 rsq     standard   0.431    25  0.0105 Preprocess~
##  9    0.1                   1 rmse    standard   4.24     25  0.0601 Preprocess~
## 10    0.1                   1 rsq     standard   0.431    25  0.0105 Preprocess~
## # ... with 40 more rows</code></pre>
<p></br></p>
<p><em>Plot model performance using autoplot()</em></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>dtree_resample <span class="sc">%&gt;%</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>()</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-24-1.png" width="672" /> </br></p>
<p><em>Showing and selecting best performing Models</em></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Showing best performing tree models</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>dtree_resample <span class="sc">%&gt;%</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">show_best</span>(<span class="at">n=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## Warning: No value of `metric` was given; metric &#39;rmse&#39; will be used.</code></pre>
<pre><code>## # A tibble: 1 x 8
##   cost_complexity tree_depth .metric .estimator  mean     n std_err .config     
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       
## 1    0.0000000001          8 rmse    standard    1.38    25  0.0612 Preprocesso~</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Selects best performing model</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>best_tree <span class="ot">&lt;-</span> dtree_resample <span class="sc">%&gt;%</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select_best</span>()</span></code></pre></div>
<pre><code>## Warning: No value of `metric` was given; metric &#39;rmse&#39; will be used.</code></pre>
<p><em>This shows a tree with depth = 7 is the best performing models (RMSE = 1.33; STE = 0.040). This model performs much better than the null model.</em></p>
<p></br></p>
<p><em>Creating final fit based on best model permutation and plotting predicted values from that final fit model</em></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>dtree_final_wf <span class="ot">&lt;-</span> </span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>  dtree_wf <span class="sc">%&gt;%</span> </span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(best_tree)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>dtree_final_wf</span></code></pre></div>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: decision_tree()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_dummy()
## * step_zv()
## * step_normalize()
## 
## -- Model -----------------------------------------------------------------------
## Decision Tree Model Specification (regression)
## 
## Main Arguments:
##   cost_complexity = 1e-10
##   tree_depth = 8
## 
## Computational engine: rpart</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create workflow for fitting model to train_data2 predictions</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>dtree_final_fit <span class="ot">&lt;-</span> </span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>  dtree_final_wf <span class="sc">%&gt;%</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(train_data) </span></code></pre></div>
</div>
<div id="calculating-residuals-and-ploting-actual-vs.-predicted-values" class="section level4">
<h4>Calculating residuals and ploting Actual vs. Predicted values</h4>
<p></br></p>
<p><em>Calculating residuals manually.</em></p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>dtree_residuals <span class="ot">&lt;-</span> dtree_final_fit <span class="sc">%&gt;%</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(train_data) <span class="sc">%&gt;%</span> <span class="co">#use augment() to make predictions from train data</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">c</span>(.pred, avg_time_lap)) <span class="sc">%&gt;%</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.resid =</span> avg_time_lap <span class="sc">-</span> .pred) <span class="co">#calculate residuals and make new row.</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>dtree_residuals</span></code></pre></div>
<pre><code>## # A tibble: 189 x 3
##    .pred avg_time_lap   .resid
##    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;
##  1  30.1         30.1 -0.00727
##  2  26.8         29.6  2.80   
##  3  33.2         33.1 -0.154  
##  4  22.0         22.2  0.154  
##  5  35.4         34.8 -0.666  
##  6  29.7         31.0  1.30   
##  7  23.8         23.9  0.113  
##  8  26.7         26.4 -0.308  
##  9  32.4         33.8  1.43   
## 10  36.3         36.4  0.0600 
## # ... with 179 more rows</code></pre>
<p><em>model predictions from tuned model vs actual outcomes</em></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>dtree_pred_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dtree_residuals, </span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">aes</span>(<span class="at">x =</span> avg_time_lap, </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Actual: Decision Tree&quot;</span>, </span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Actual Average Time Lap&quot;</span>, </span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Average Time Lap Prediction&quot;</span>)</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>dtree_pred_plot</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-28-1.png" width="672" /> <em>plot residuals vs predictions</em></p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>dtree_residual_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dtree_residuals, </span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">aes</span>(<span class="at">y =</span> .resid, </span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">x =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Residuals: Decision Tree&quot;</span>, </span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Average Time Lap Prediction&quot;</span>, </span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dtree_residual_plot) <span class="co">#view plot</span></span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p></br></p>
</div>
</div>
<div id="b-lasso-model" class="section level3">
<h3>b) LASSO Model</h3>
<p></br></p>
<p><em>Specifying The Model: LASSO</em></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>lasso_mod <span class="ot">&lt;-</span> </span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span></code></pre></div>
<p></br></p>
<p><em>Creating a Workflow: LASSO</em></p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>lasso_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(lasso_mod) <span class="sc">%&gt;%</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(ATL.recipe)</span></code></pre></div>
<p></br></p>
<p><em>Create Tuning Grid: LASSO</em></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>lasso_grid <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">penalty =</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="at">length.out =</span> <span class="dv">30</span>))</span></code></pre></div>
<p></br></p>
<p><em>Cross Validation with tune_grid(): LASSO</em></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>lasso_resample <span class="ot">&lt;-</span> </span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>  lasso_wf <span class="sc">%&gt;%</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(<span class="at">resamples =</span> fold,</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">grid =</span> lasso_grid,</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">verbose =</span> <span class="cn">FALSE</span>, <span class="at">save_pred =</span> <span class="cn">TRUE</span>),</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">metrics =</span> <span class="fu">metric_set</span>(rmse))</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>lasso_resample <span class="sc">%&gt;%</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 30 x 7
##    penalty .metric .estimator  mean     n std_err .config              
##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1 0.001   rmse    standard    1.10    25  0.0195 Preprocessor1_Model01
##  2 0.00127 rmse    standard    1.10    25  0.0195 Preprocessor1_Model02
##  3 0.00161 rmse    standard    1.10    25  0.0195 Preprocessor1_Model03
##  4 0.00204 rmse    standard    1.10    25  0.0195 Preprocessor1_Model04
##  5 0.00259 rmse    standard    1.10    25  0.0195 Preprocessor1_Model05
##  6 0.00329 rmse    standard    1.10    25  0.0196 Preprocessor1_Model06
##  7 0.00418 rmse    standard    1.09    25  0.0196 Preprocessor1_Model07
##  8 0.00530 rmse    standard    1.09    25  0.0196 Preprocessor1_Model08
##  9 0.00672 rmse    standard    1.09    25  0.0196 Preprocessor1_Model09
## 10 0.00853 rmse    standard    1.09    25  0.0197 Preprocessor1_Model10
## # ... with 20 more rows</code></pre>
<p></br></p>
<p><em>Plot model performance using autoplot()</em></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot of actual train_data</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>lasso_resample <span class="sc">%&gt;%</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>()</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p><em>Showing and selecting best performing Models</em></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Showing best performing tree models</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>lasso_resample <span class="sc">%&gt;%</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">show_best</span>()</span></code></pre></div>
<pre><code>## # A tibble: 5 x 7
##   penalty .metric .estimator  mean     n std_err .config              
##     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1  0.0574 rmse    standard    1.06    25  0.0230 Preprocessor1_Model18
## 2  0.0452 rmse    standard    1.06    25  0.0216 Preprocessor1_Model17
## 3  0.0356 rmse    standard    1.07    25  0.0207 Preprocessor1_Model16
## 4  0.0728 rmse    standard    1.07    25  0.0250 Preprocessor1_Model19
## 5  0.0281 rmse    standard    1.07    25  0.0202 Preprocessor1_Model15</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Selects best performing model</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>best_lasso <span class="ot">&lt;-</span> lasso_resample <span class="sc">%&gt;%</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select_best</span>()</span></code></pre></div>
<p><em>This shows that model 18 is the best performing models (RMSE = 1.10; STE = 0.025). It performs better than the null model, indicating that it fits the data with some sort of relationship.</em></p>
<p><em>Creating final fit based on best model permutation and plotting predicted values from that final fit model</em></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>lasso_final_wf <span class="ot">&lt;-</span> </span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>  lasso_wf <span class="sc">%&gt;%</span> </span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(best_lasso)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>lasso_final_wf</span></code></pre></div>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: linear_reg()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_dummy()
## * step_zv()
## * step_normalize()
## 
## -- Model -----------------------------------------------------------------------
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.0573615251044868
##   mixture = 1
## 
## Computational engine: glmnet</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create workflow for fitting model to train_data2 predictions</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>lasso_final_fit <span class="ot">&lt;-</span> </span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>  lasso_final_wf <span class="sc">%&gt;%</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(train_data) </span></code></pre></div>
<p><em>Calculating residuals manually.</em></p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>lasso_residuals <span class="ot">&lt;-</span> lasso_final_fit <span class="sc">%&gt;%</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(train_data) <span class="sc">%&gt;%</span> <span class="co">#use augment() to make predictions from train data</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">c</span>(.pred, avg_time_lap)) <span class="sc">%&gt;%</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.resid =</span> avg_time_lap <span class="sc">-</span> .pred) <span class="co">#calculate residuals and make new row.</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>lasso_residuals</span></code></pre></div>
<pre><code>## # A tibble: 189 x 3
##    .pred avg_time_lap .resid
##    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;
##  1  30.9         30.1 -0.796
##  2  27.3         29.6  2.32 
##  3  32.9         33.1  0.152
##  4  21.9         22.2  0.291
##  5  35.8         34.8 -1.07 
##  6  31.6         31.0 -0.650
##  7  24.0         23.9 -0.140
##  8  26.5         26.4 -0.175
##  9  33.1         33.8  0.715
## 10  36.0         36.4  0.370
## # ... with 179 more rows</code></pre>
<p><em>model predictions from tuned model vs actual outcomes</em></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>lasso_pred_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lasso_residuals, </span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">aes</span>(<span class="at">x =</span> avg_time_lap, </span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Actual: LASSO&quot;</span>, </span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Average Lap Time&quot;</span>, </span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Average Lap Time Prediction&quot;</span>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>lasso_pred_plot</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-38-1.png" width="672" /> <em>plot residuals vs predictions</em></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>lasso_residual_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lasso_residuals, </span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">aes</span>(<span class="at">y =</span> .resid, </span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">x =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Residuals: LASSO&quot;</span>, </span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Average Lap Time Prediction&quot;</span>, </span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_residual_plot) <span class="co">#view plot</span></span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p></br></p>
</div>
<div id="c-random-forest" class="section level3">
<h3>c) Random Forest</h3>
<p></br></p>
<p><em>Create function to detect cores for Random Forest Model computation</em></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>cores <span class="ot">&lt;-</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>()</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>cores</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<p><em>Specifying The Model: Random Forest</em></p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>rf_mod <span class="ot">&lt;-</span> </span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rand_forest</span>(<span class="at">mtry =</span> <span class="fu">tune</span>(), <span class="at">min_n =</span> <span class="fu">tune</span>(), <span class="at">trees =</span> <span class="dv">1000</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>, <span class="at">num.threads =</span> cores) <span class="sc">%&gt;%</span> </span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<p></br></p>
<p><em>Creating a Workflow: Random Forest</em></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>rf_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(rf_mod) <span class="sc">%&gt;%</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(ATL.recipe)</span></code></pre></div>
<p></br></p>
<p><em>Create Tuning Grid: Random Forest</em></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>rf_grid  <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">mtry =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>), <span class="at">min_n =</span> <span class="fu">c</span>(<span class="dv">40</span>,<span class="dv">50</span>,<span class="dv">60</span>), <span class="at">trees =</span> <span class="fu">c</span>(<span class="dv">500</span>,<span class="dv">1000</span>)  )</span></code></pre></div>
<p></br></p>
<p><em>Cross Validation with tune_grid(): Random Forest</em></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>rf_resample <span class="ot">&lt;-</span> </span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>  rf_wf <span class="sc">%&gt;%</span> </span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(fold,</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">grid =</span> <span class="dv">25</span>,</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>),</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">metrics =</span> <span class="fu">metric_set</span>(rmse))</span></code></pre></div>
<pre><code>## i Creating pre-processing data to finalize unknown parameter: mtry</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>rf_resample <span class="sc">%&gt;%</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 25 x 8
##     mtry min_n .metric .estimator  mean     n std_err .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1    39     4 rmse    standard   0.920    25  0.0363 Preprocessor1_Model01
##  2     8    17 rmse    standard   1.79     25  0.0402 Preprocessor1_Model02
##  3    22    15 rmse    standard   1.12     25  0.0216 Preprocessor1_Model03
##  4     2    13 rmse    standard   3.84     25  0.0779 Preprocessor1_Model04
##  5     5    32 rmse    standard   2.54     25  0.0576 Preprocessor1_Model05
##  6    12    40 rmse    standard   1.94     25  0.0453 Preprocessor1_Model06
##  7    37    25 rmse    standard   1.38     25  0.0215 Preprocessor1_Model07
##  8    52    13 rmse    standard   0.937    25  0.0360 Preprocessor1_Model08
##  9    49    35 rmse    standard   1.80     25  0.0278 Preprocessor1_Model09
## 10    10    10 rmse    standard   1.43     25  0.0308 Preprocessor1_Model10
## # ... with 15 more rows</code></pre>
<p></br></p>
<p><em>Plot model performance using autoplot()</em></p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot of actual train_data2</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>rf_resample <span class="sc">%&gt;%</span></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>()</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p><em>Showing and selecting best performing Models</em></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Showing best performing tree models</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>rf_resample <span class="sc">%&gt;%</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">show_best</span>()</span></code></pre></div>
<pre><code>## # A tibble: 5 x 8
##    mtry min_n .metric .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1    30     6 rmse    standard   0.919    25  0.0337 Preprocessor1_Model11
## 2    39     4 rmse    standard   0.920    25  0.0363 Preprocessor1_Model01
## 3    28     8 rmse    standard   0.930    25  0.0330 Preprocessor1_Model18
## 4    52    13 rmse    standard   0.937    25  0.0360 Preprocessor1_Model08
## 5    20     3 rmse    standard   0.981    25  0.0284 Preprocessor1_Model17</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Selects best performing model</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>best_rf <span class="ot">&lt;-</span> rf_resample <span class="sc">%&gt;%</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select_best</span>()</span></code></pre></div>
<p><em>This shows that “mtry 52” is the best performing model (RMSE = 0.94; STE = 0.032). This performs better than the null model, indicating the model fits the data well.</em></p>
<p><em>Creating final fit based on best model permutation and plotting predicted values from that final fit model</em></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>rf_final_wf <span class="ot">&lt;-</span> </span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>  rf_wf <span class="sc">%&gt;%</span> </span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(best_rf)</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>rf_final_wf</span></code></pre></div>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_dummy()
## * step_zv()
## * step_normalize()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = 30
##   trees = 1000
##   min_n = 6
## 
## Engine-Specific Arguments:
##   num.threads = cores
## 
## Computational engine: ranger</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create workflow for fitting model to train_data predictions</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>rf_final_fit <span class="ot">&lt;-</span> </span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>  rf_final_wf <span class="sc">%&gt;%</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(train_data) </span></code></pre></div>
<p><em>Calculating residuals manually.</em></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>rf_residuals <span class="ot">&lt;-</span> rf_final_fit <span class="sc">%&gt;%</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(train_data) <span class="sc">%&gt;%</span> <span class="co">#use augment() to make predictions from train data</span></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">c</span>(.pred, avg_time_lap)) <span class="sc">%&gt;%</span></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.resid =</span> avg_time_lap <span class="sc">-</span> .pred) <span class="co">#calculate residuals and make new row.</span></span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>rf_residuals</span></code></pre></div>
<pre><code>## # A tibble: 189 x 3
##    .pred avg_time_lap   .resid
##    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;
##  1  30.1         30.1  0.00264
##  2  28.4         29.6  1.24   
##  3  33.2         33.1 -0.101  
##  4  22.1         22.2  0.0948 
##  5  35.1         34.8 -0.354  
##  6  31.0         31.0  0.0205 
##  7  23.9         23.9 -0.00944
##  8  26.5         26.4 -0.101  
##  9  33.1         33.8  0.683  
## 10  36.3         36.4  0.0533 
## # ... with 179 more rows</code></pre>
<p><em>model predictions from tuned model vs actual outcomes</em></p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>rf_pred_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rf_residuals, </span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">aes</span>(<span class="at">x =</span> avg_time_lap, </span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Actual: Random Forest&quot;</span>, </span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Average Lap Time Actual&quot;</span>, </span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Average Lap Time Prediction&quot;</span>)</span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a>rf_pred_plot</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-49-1.png" width="672" /> <em>plot residuals vs predictions</em></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>rf_residual_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rf_residuals, </span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">aes</span>(<span class="at">y =</span> .resid, </span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">x =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Residuals: Random Forest&quot;</span>, </span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Average Lap Time Prediction&quot;</span>, </span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf_residual_plot) <span class="co">#view plot</span></span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p></br></p>
</div>
<div id="d-ensemble-model" class="section level3">
<h3>d) Ensemble Model</h3>
<p><em>Based on the models run above, I tend to like the LASSO model because of the penalty and removal associated with poorly predicting variables. However, the random forest model had the best RMSE value when compared to the null model for Average Lap Time. Therefore, I want to build an ensemble model with LASSO and Random Forest to see if the combination fits the data any better.</em></p>
<p><em>Note, that the ensemble model already has recipes, workflows, etc. for LASSO and Random Forest Models created above. All that needs to be done is combine them.</em></p>
<p></br></p>
<p><em>edit resampling definition for both random forest and LASSO to include proper control arguments</em></p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="co">#random forest</span></span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>rf_resample2 <span class="ot">&lt;-</span> </span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>  rf_wf <span class="sc">%&gt;%</span> </span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(fold,</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">grid =</span> <span class="dv">25</span>,</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">control =</span> <span class="fu">control_stack_grid</span>(),</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">metrics =</span> <span class="fu">metric_set</span>(rmse))</span></code></pre></div>
<pre><code>## i Creating pre-processing data to finalize unknown parameter: mtry</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>rf_resample2 <span class="sc">%&gt;%</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 25 x 8
##     mtry min_n .metric .estimator  mean     n std_err .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1    47    39 rmse    standard   1.91     25  0.0297 Preprocessor1_Model01
##  2    17    17 rmse    standard   1.28     25  0.0231 Preprocessor1_Model02
##  3    46     8 rmse    standard   0.912    25  0.0380 Preprocessor1_Model03
##  4    53    30 rmse    standard   1.66     25  0.0238 Preprocessor1_Model04
##  5    29    23 rmse    standard   1.33     25  0.0220 Preprocessor1_Model05
##  6    26    33 rmse    standard   1.59     25  0.0297 Preprocessor1_Model06
##  7    41    25 rmse    standard   1.38     25  0.0200 Preprocessor1_Model07
##  8    14     5 rmse    standard   1.12     25  0.0271 Preprocessor1_Model08
##  9    21     9 rmse    standard   0.991    25  0.0278 Preprocessor1_Model09
## 10    10    36 rmse    standard   1.97     25  0.0464 Preprocessor1_Model10
## # ... with 15 more rows</code></pre>
<p></br></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co">#LASSO</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>lasso_resample2 <span class="ot">&lt;-</span> </span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>  lasso_wf <span class="sc">%&gt;%</span></span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(<span class="at">resamples =</span> fold,</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">grid =</span> lasso_grid,</span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">control =</span> <span class="fu">control_stack_grid</span>(),</span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">metrics =</span> <span class="fu">metric_set</span>(rmse))</span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-9"><a href="#cb120-9" aria-hidden="true" tabindex="-1"></a>lasso_resample2 <span class="sc">%&gt;%</span></span>
<span id="cb120-10"><a href="#cb120-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 30 x 7
##    penalty .metric .estimator  mean     n std_err .config              
##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1 0.001   rmse    standard    1.10    25  0.0195 Preprocessor1_Model01
##  2 0.00127 rmse    standard    1.10    25  0.0195 Preprocessor1_Model02
##  3 0.00161 rmse    standard    1.10    25  0.0195 Preprocessor1_Model03
##  4 0.00204 rmse    standard    1.10    25  0.0195 Preprocessor1_Model04
##  5 0.00259 rmse    standard    1.10    25  0.0195 Preprocessor1_Model05
##  6 0.00329 rmse    standard    1.10    25  0.0196 Preprocessor1_Model06
##  7 0.00418 rmse    standard    1.09    25  0.0196 Preprocessor1_Model07
##  8 0.00530 rmse    standard    1.09    25  0.0196 Preprocessor1_Model08
##  9 0.00672 rmse    standard    1.09    25  0.0196 Preprocessor1_Model09
## 10 0.00853 rmse    standard    1.09    25  0.0197 Preprocessor1_Model10
## # ... with 20 more rows</code></pre>
<p><em>Stacking data with stack package</em></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>clean2_df_stack <span class="ot">&lt;-</span> </span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stacks</span>() <span class="sc">%&gt;%</span></span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_candidates</span>(lasso_resample2) <span class="sc">%&gt;%</span></span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_candidates</span>(rf_resample2)</span></code></pre></div>
<pre><code>## Warning: Predictions from 1 candidate were identical to those from existing
## * candidates and were removed from the data stack.</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>clean2_df_stack</span></code></pre></div>
<pre><code>## # A data stack with 2 model definitions and 54 candidate members:
## #   lasso_resample2: 29 model configurations
## #   rf_resample2: 25 model configurations
## # Outcome: avg_time_lap (numeric)</code></pre>
<p><em>Before fitting our stack to our data, it is usually good to use the blend_prediction() function to assess how the model output will be combined in the final prediction. This is done by fitting a LASSO model on the data stack, predicting the true assessment set outcome using the predictions from each of the candidate members.</em></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>ensemble_mod <span class="ot">&lt;-</span></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>  clean2_df_stack <span class="sc">%&gt;%</span></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">blend_predictions</span>()</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(ensemble_mod)</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p><em>Fitting the stack for our LASSO/RF Ensemble model to our train data using the blended predication model.</em></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>ensemble_mod <span class="ot">&lt;-</span></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>  ensemble_mod <span class="sc">%&gt;%</span></span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit_members</span>()</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>ensemble_mod</span></code></pre></div>
<pre><code>## -- A stacked ensemble model -------------------------------------</code></pre>
<pre><code>## 
## Out of 54 possible candidate members, the ensemble retained 3.
## Penalty: 1e-06.
## Mixture: 1.</code></pre>
<pre><code>## 
## The 3 highest weighted members are:</code></pre>
<pre><code>## # A tibble: 3 x 3
##   member               type        weight
##   &lt;chr&gt;                &lt;chr&gt;        &lt;dbl&gt;
## 1 rf_resample2_1_03    rand_forest 0.663 
## 2 lasso_resample2_1_01 linear_reg  0.253 
## 3 rf_resample2_1_17    rand_forest 0.0935</code></pre>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co">#using collect_parameters function to see which individual model was used to assign which stacking coefficient.</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_parameters</span>(ensemble_mod, <span class="st">&quot;lasso_resample2&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 30 x 3
##    member               penalty   coef
##    &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;
##  1 lasso_resample2_1_01 0.001    0.253
##  2 lasso_resample2_1_02 0.00127 NA    
##  3 lasso_resample2_1_03 0.00161  0    
##  4 lasso_resample2_1_04 0.00204  0    
##  5 lasso_resample2_1_05 0.00259  0    
##  6 lasso_resample2_1_06 0.00329  0    
##  7 lasso_resample2_1_07 0.00418  0    
##  8 lasso_resample2_1_08 0.00530  0    
##  9 lasso_resample2_1_09 0.00672  0    
## 10 lasso_resample2_1_10 0.00853  0    
## # ... with 20 more rows</code></pre>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_parameters</span>(ensemble_mod, <span class="st">&quot;rf_resample2&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 25 x 4
##    member             mtry min_n  coef
##    &lt;chr&gt;             &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
##  1 rf_resample2_1_01    47    39 0    
##  2 rf_resample2_1_02    17    17 0    
##  3 rf_resample2_1_03    46     8 0.663
##  4 rf_resample2_1_04    53    30 0    
##  5 rf_resample2_1_05    29    23 0    
##  6 rf_resample2_1_06    26    33 0    
##  7 rf_resample2_1_07    41    25 0    
##  8 rf_resample2_1_08    14     5 0    
##  9 rf_resample2_1_09    21     9 0    
## 10 rf_resample2_1_10    10    36 0    
## # ... with 15 more rows</code></pre>
<p><em>This shows that rf_resamble2-1-03 is the best performing model (weight = 0.488), which is a random forest model. However, the summary reported that only 4 models total out of 52 total possibilities. This seems like a red flag to me, possibly indicating that I don’t have enough data. Therefore, I will most likely not use it.</em></p>
</div>
</div>
<div id="model-evaluation" class="section level2">
<h2>Model Evaluation</h2>
<p><em>Based on the comparison of RMSE scores between the four models and the null model, analysis of fit made by each model, and prediction fitting, I would like to use the LASSO model on the test data. The Random Forest had the best over all RMSE, but the LASSO model most likely picked a combination of variables that best predicted the outcome by dropping variables with penalties &lt;/= 0. As stated above, the Ensemble model is ruled out because there is likely not enough data to make the Ensemble model perform well.</em></p>
<p></br></p>
</div>
<div id="test-data-modeling" class="section level2">
<h2>Test Data Modeling</h2>
<p></br></p>
<div id="lasso-model" class="section level3">
<h3>LASSO Model</h3>
<p></br></p>
<p><em>Specifying The Model: LASSO</em></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>lasso_mod <span class="ot">&lt;-</span> </span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span></code></pre></div>
<p><em>Creating new recipe for LASSO modeling of test data</em></p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>ATL.recipe2 <span class="ot">&lt;-</span> </span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">recipe</span>(avg_time_lap <span class="sc">~</span> ., <span class="at">data =</span> test_data) <span class="sc">%&gt;%</span></span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_zv</span>(<span class="fu">all_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-7"><a href="#cb137-7" aria-hidden="true" tabindex="-1"></a>ATL.recipe2</span></code></pre></div>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          5
## 
## Operations:
## 
## Dummy variables from all_nominal(), -all_outcomes()
## Zero variance filter on all_predictors()
## Centering and scaling for all_predictors()</code></pre>
<p></br></p>
<p><em>Creating a Workflow: LASSO</em></p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>lasso_wf2 <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(lasso_mod) <span class="sc">%&gt;%</span></span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(ATL.recipe2)</span></code></pre></div>
<p></br></p>
<p><em>Create Tuning Grid: LASSO</em></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>lasso_grid <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">penalty =</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="at">length.out =</span> <span class="dv">30</span>))</span></code></pre></div>
<p></br></p>
<p><em>Cross Validation with tune_grid(): LASSO</em></p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>lasso_resample2 <span class="ot">&lt;-</span> </span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>  lasso_wf2 <span class="sc">%&gt;%</span></span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(<span class="at">resamples =</span> fold,</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">grid =</span> lasso_grid,</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">verbose =</span> <span class="cn">FALSE</span>, <span class="at">save_pred =</span> <span class="cn">TRUE</span>),</span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">metrics =</span> <span class="fu">metric_set</span>(rmse))</span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a>lasso_resample2 <span class="sc">%&gt;%</span></span>
<span id="cb141-9"><a href="#cb141-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 30 x 7
##    penalty .metric .estimator  mean     n std_err .config              
##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1 0.001   rmse    standard    1.10    25  0.0195 Preprocessor1_Model01
##  2 0.00127 rmse    standard    1.10    25  0.0195 Preprocessor1_Model02
##  3 0.00161 rmse    standard    1.10    25  0.0195 Preprocessor1_Model03
##  4 0.00204 rmse    standard    1.10    25  0.0195 Preprocessor1_Model04
##  5 0.00259 rmse    standard    1.10    25  0.0195 Preprocessor1_Model05
##  6 0.00329 rmse    standard    1.10    25  0.0196 Preprocessor1_Model06
##  7 0.00418 rmse    standard    1.09    25  0.0196 Preprocessor1_Model07
##  8 0.00530 rmse    standard    1.09    25  0.0196 Preprocessor1_Model08
##  9 0.00672 rmse    standard    1.09    25  0.0196 Preprocessor1_Model09
## 10 0.00853 rmse    standard    1.09    25  0.0197 Preprocessor1_Model10
## # ... with 20 more rows</code></pre>
<p></br></p>
<p><em>Plot model performance using autoplot()</em></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot of actual train_data</span></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>lasso_resample2 <span class="sc">%&gt;%</span></span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>()</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p><em>Showing and selecting best performing Models</em></p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Showing best performing tree models</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>lasso_resample2 <span class="sc">%&gt;%</span></span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">show_best</span>()</span></code></pre></div>
<pre><code>## # A tibble: 5 x 7
##   penalty .metric .estimator  mean     n std_err .config              
##     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1  0.0574 rmse    standard    1.06    25  0.0230 Preprocessor1_Model18
## 2  0.0452 rmse    standard    1.06    25  0.0216 Preprocessor1_Model17
## 3  0.0356 rmse    standard    1.07    25  0.0207 Preprocessor1_Model16
## 4  0.0728 rmse    standard    1.07    25  0.0250 Preprocessor1_Model19
## 5  0.0281 rmse    standard    1.07    25  0.0202 Preprocessor1_Model15</code></pre>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Selects best performing model</span></span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a>best_lasso2 <span class="ot">&lt;-</span> lasso_resample2 <span class="sc">%&gt;%</span></span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select_best</span>()</span></code></pre></div>
<p><em>This shows that model 16 is the best performing models (RMSE = 1.035; STE = 0.024). It performs better than the null model, indicating that it fits the data with some sort of relationship.</em></p>
<p><em>Creating final fit based on best model permutation and plotting predicted values from that final fit model</em></p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>lasso_final_wf2 <span class="ot">&lt;-</span> </span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a>  lasso_wf2 <span class="sc">%&gt;%</span> </span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(best_lasso2)</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a>lasso_final_wf2</span></code></pre></div>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: linear_reg()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_dummy()
## * step_zv()
## * step_normalize()
## 
## -- Model -----------------------------------------------------------------------
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.0573615251044868
##   mixture = 1
## 
## Computational engine: glmnet</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create workflow for fitting model to test_data predictions</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>lasso_final_fit2 <span class="ot">&lt;-</span> </span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a>  lasso_final_wf2 <span class="sc">%&gt;%</span></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(test_data) </span></code></pre></div>
<p><em>Calculating residuals manually.</em></p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>lasso_residuals2 <span class="ot">&lt;-</span> lasso_final_fit2 <span class="sc">%&gt;%</span></span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(test_data) <span class="sc">%&gt;%</span> <span class="co">#use augment() to make predictions from train data</span></span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">c</span>(.pred, avg_time_lap)) <span class="sc">%&gt;%</span></span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.resid =</span> avg_time_lap <span class="sc">-</span> .pred) <span class="co">#calculate residuals and make new row.</span></span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a>lasso_residuals2</span></code></pre></div>
<pre><code>## # A tibble: 64 x 3
##    .pred avg_time_lap  .resid
##    &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;
##  1  28.8         28.7 -0.120 
##  2  29.8         30.0  0.173 
##  3  29.8         30.1  0.273 
##  4  32.4         32.4  0.0827
##  5  33.0         32.8 -0.186 
##  6  33.0         33.5  0.514 
##  7  33.3         33.7  0.409 
##  8  31.9         30.4 -1.50  
##  9  32.8         31.0 -1.75  
## 10  32.5         31.6 -0.885 
## # ... with 54 more rows</code></pre>
<p><em>model predictions from tuned model vs actual outcomes</em></p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>lasso_pred_plot2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lasso_residuals2, </span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">aes</span>(<span class="at">x =</span> avg_time_lap, </span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Actual: LASSO Test&quot;</span>, </span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Average Lap Time&quot;</span>, </span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Average Lap Time Prediction&quot;</span>)</span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a>lasso_pred_plot2</span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p><em>plot residuals vs predictions</em></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>lasso_residual_plot2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lasso_residuals2, </span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">aes</span>(<span class="at">y =</span> .resid, </span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">x =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Predictions vs Residuals: LASSO Test&quot;</span>, </span>
<span id="cb153-6"><a href="#cb153-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Average Lap Time Prediction&quot;</span>, </span>
<span id="cb153-7"><a href="#cb153-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb153-8"><a href="#cb153-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_residual_plot2) <span class="co">#view plot</span></span></code></pre></div>
<p><img src="tidytuesday_exercise2_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<p></br></p>
</div>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<p><em>If you haven’t already repressed it, think back to June 2020. The COVID-19 pandemic is in full swing, lockdown orders are still in place in areas where the virus is spreading at very high rates. You are terrified of interacting with people and one by one you favorite tv shows, movie franchises, and sports are disappearing. While some of us decided to throw our lot in to hiking and DIY crafts, it seems others found excitement in a new contactless sport: marble racing. In this tidy tuesday, I will be taking on the role of an aspiring pandemic ESPN analyst to cover the the Jelle’s Marble Run competition. My goal will be to use the 2020 season’s data to assess that makes a winning marble. This will ultimately be done through the use of Machine Learning models.</em></p>
<p><em>During this exercise, I loaded raw data for the Jelle’s Marble Run 2020 season. BAsed on observations made in the loaded data, I cleaned the data down to essentially six variables of interest: site, marble name, team name, track length, number of laps, and average lap time. I chose average lap time as my outcome in an effort to try and see if we could predict marble performance based on the other five predictors.</em></p>
<p><em>Exploration of the data basically showed that there was some sort of trend between average lap time and all variables except the team that marble belongs to number of laps in race. Given this information, I included all variables into four machine learning models to see if we could make a complex model to predict marble performance.</em></p>
<p><em>Results of the four models are as follows:</em></p>
<p><em>Tree Model shows a tree with depth = 7 is the best performing models (RMSE = 1.33; STE = 0.040). This model performs much better than the null model.</em> <em>Lasso shows that model 18 is the best performing models (RMSE = 1.10; STE = 0.025). It performs better than the null model, indicating that it fits the data with some sort of relationship.</em> <em>Random Forest shows that “mtry 52” is the best performing model (RMSE = 0.94; STE = 0.032). This performs better than the null model, indicating the model fits the data well.</em> <em>Ensemble model shows that rf_resamble2-1-03 is the best performing model (weight = 0.488), which is a random forest model. However, the summary reported that only 4 models total out of 52 total possibilities. This seems like a red flag to me, possibly indicating that I don’t have enough data. Therefore, I will most likely not use it.</em></p>
<p><em>At this point, I concluded that based on the comparison of RMSE scores between the four models and the null model, analysis of fit made by each model, and prediction fitting, I would like to use the LASSO model on the test data. The Random Forest had the best over all RMSE, but the LASSO model most likely picked a combination of variables that best predicted the outcome by dropping variables with penalties &lt;/= 0. As stated above, the Ensemble model is ruled out because there is likely not enough data to make the Ensemble model perform well.</em></p>
<p><em>Finally, I re-ran a LASSO model, this time with the test data. Results indicated that model 16 was the best performing model (RMSE = 1.035; STE = 0.024). It performs better than the null model, indicating that it fits the data with some sort of relationship. The RMSE and standard deviation for this LASSO model for test data is consistent with that of the train data, showing that predictions with good fit can be made and reproduced using the LASSO model.</em></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
