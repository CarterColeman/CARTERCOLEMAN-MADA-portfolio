---
title: "Tidy Tuesday"
output: 
  html_document:
    toc: FALSE
---

## Welcome to my TidyTuesday Exercise. This week focuses on cleaning and exploring paper authorship data from the NBER.

</br>

#### Load needed packages.
```{r}
library(tidyverse)
library(tidyr)
library(dplyr)
library(readr)
library(here)
```

#### Load the TidyTuesday Data
```{r}
papers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/papers.csv')
authors <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/authors.csv')
programs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/programs.csv')
paper_authors <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_authors.csv')
paper_programs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_programs.csv')
```
#### Initial cleaning from Tidy Tuesday Repo. This basically joins excel files containing separate sets of variables into one chart. From here, whe can take a wholistic look at the data and see if anything is interesting. 
```{r}
joined_df <- 
  left_join(papers, paper_authors) %>% 
  left_join(authors) %>% 
  left_join(paper_programs) %>%
  left_join(programs)%>% 
  mutate(
    catalogue_group = str_sub(paper, 1, 1),
    catalogue_group = case_when(
      catalogue_group == "h" ~ "Historical",
      catalogue_group == "t" ~ "Technical",
      catalogue_group == "w" ~ "General"
    ),
    .after = paper
  ) 

joined_df
```

#### We can also use the glimpse function to check outr our data sets. Since all the data is combined already, lets take a look at the joined_df.
```{r}
glimpse(joined_df)
```

</br>

### At a glance, I see that there are 130,081 observations of 12 Variables. Additionally, the most of the variables seem to be categorical, with the exception of year and month. Therefore, I think it would make sense to see how some of these catergories change from year to year or decade to decade. To narrow it down, I am going to get rid of columns I think are redundant.

</br>

#### Remove user and shorthand variables: These rows give information on author identity or provide shorthand notation of other columns. Since we have full names, these rows names aren't useful.
```{r}
clean_df <- 
  joined_df %>%
  select(-"user_nber", -"user_repec", -"program")

view(clean_df)
```

</br>

#### Now lets take a look at the NAs.
```{r}
clean_df %>%
  select(everything()) %>%  
  summarise_all(funs(sum(is.na(.))))
```
### The only NAs found are on the program description and program category variables. While they seem like large numbers, that is only about 0.4% and 1% of observations in each column. Therefore, I think its fine to keep them. 

</br>

### Arbitrarily, I just wank to pick two variables to look at to see if anything interesting is going on. Let's maybe do catalogue group over time? More specifically, let's look at count contribution of papers to each catalouge group as a function of time in years.

```{r}
catalouge_plot <- clean_df %>%
  ggplot(aes(x=year, fill=catalogue_group)) + geom_bar()

print(catalouge_plot)
```

</br>

### Interesting. First, it looks like there is somewhat of an exponential trend in number of papers written per year over time. Maybe it is a function of staff size? However, it looks like most papers are listed under the general catalogue group. This isn't an interesting result, so we will likely not explore the catalogue group variable.

</br>

## Questions: Does the number of authors increase through time? Is there a correlation between number of authors and papers published? How efficient are these authors at producing papers?

</br>

### Obviously, you would expect number of papers to increase with increasing number of authors; more authors means more papers being produced per unit time. However, this gives us a chance to run some simple linear regression stats to see what the slope of the papers-to-authors relationship is (i.e. how efficient are these authors at writing papers).

</br>

### Let's create data sets with number of papers through time and number of authors through time.

Create two new data that contain either authors through time or papers through time.
```{r}
Author_df <- clean_df %>%
  select("year", "name",)

Paper_df <- clean_df %>%
  select("year", "paper")
```

#### Let's take a look at those new tables.
```{r}
glimpse(Author_df)

glimpse(Paper_df)
```


</br>

#### Now we can make a bar graph of both number of authors and papers through time.

Authors
```{r}
Author_df <- distinct(Author_df) #Removes duplicates to avoid double counts.

Authors_plot <- Author_df %>%
  ggplot(aes(x=year)) + 
  geom_bar(fill = "red")

print(Authors_plot)
```

### Okay, as predicted, the number of Authors does increase through time. Thats good for NBER, as it shows the company is growing. Now let's plot the same chart with log(autors) to see what the linear relationship looks like.
```{r}
log_authors_plot <- Author_df %>%
  ggplot(aes(x=year)) + 
  geom_bar(fill = "red") + scale_y_continuous(trans = "log10")

print(log_authors_plot)
```

</br>

### Let's do the same for papers over time.

Without log scale
```{r}
papers_plot <- Paper_df %>%
  ggplot(aes(x=year)) + 
  geom_bar(fill = "blue")

print(papers_plot)
```


```{r}
log_papers_plot <- Paper_df %>%
  ggplot(aes(x=year)) + 
  geom_bar(fill = "blue") + scale_y_continuous(trans = "log10")

print(log_papers_plot)
```
#### By comparing the two charts, we can see that the papers increase through time as well. But they do so to an order of magnitude greater than there are authors! That's an intersting relationship for sure! Shows that not only is NBER increasing it's staff over time, but they are publishing more papers per author over time as well. How much though?

</br>

### Now let's plot number of papers produced by number of authors per year

</br>

#### First we need to clean the data a bit more.... Namely creat new data frames that group the sums of both papers produced and authors by year.

Authors
```{r}
author_count <- as.data.frame(table(Author_df$year))

colnames(author_count) <- c("year", "count_author")
  
view(author_count) 
```

Papers
```{r}
paper_count <- as.data.frame(table(Paper_df$year))

colnames(paper_count) <- c("year", "count_paper")
  
view(paper_count)
```

Merge together
```{r}
author_efficiency <- merge(author_count, paper_count, by="year")

view(author_efficiency)
```

plot
```{r}
efficiency_plot <- author_efficiency %>%
  ggplot(aes(x=count_author, y=count_paper)) +
  geom_point()

print(efficiency_plot)
```

Now, we are going to run a linear regression just to see what the slope is.
```{r}
papers_per_author <- lm(count_paper ~ count_author, data=author_efficiency)

summary(papers_per_author)
```
#### Based on this, we are looking at, on average, about 3 papers per author since NBER's founding. 

</br>

### Finally, lets look for the year with highest per author paper production.
```{r}
author_efficiency <- author_efficiency %>%
  mutate(Paper_per_Author = count_paper / count_author)

view(author_efficiency)
```
Plot
```{r}
per_cap_paper <- author_efficiency %>%
  ggplot(aes(x=(year), y=Paper_per_Author)) +
  geom_bar(stat = "identity", fill = "red") +
  scale_x_discrete(breaks=c("1970", "1980", "1990", "2000", "2010", "2020"))
  

print(per_cap_paper)
```
</br>

### This final result shows an over all incresing trend over time with a somewhat noticable dip in the 1990s and the maximum in 2020.





